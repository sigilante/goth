# ğ”Šğ”¬ğ”±ğ”¥
### the `goth` language for machine spirits

`goth` is an LLM-native programming language designed for efficient code generation, editing, and comprehension by large language models.

This document records the design process and decisions for `goth`.  The code samples are not up-to-date with the latest release but do illustrate the design philosophy and history.

Future work:
* posits, bfloat16, TensorFloat-32
* GPU intrinsics
* SIMD intrinsics
* Memory layout
* terminal with Ani-like assistant

---

I've wondered idly for a while about what a language optimized for LLMs rather than humans would look like.  Now, the priests of the machine spirits still need to be able to read the language, so it needs a legible representation (as opposed to a blast of static).  But we can play with other parameters in order to make it legible.  In fact, a very few people have started to play this game (like [LMQL](https://lmql.ai/)).  So, what the heck, let's just build a new language from scratch over the weekend.

Our design constraints include prioritizing what LLMs are good at while avoiding what hurts their context and accuracy.  These things are favorable for LLMs:

- Pattern completion over structural templates
- Reasoning about explicit dataflow
- Type-level constraint satisfaction
- Compositional/algebraic thinking
- Dense semantic compression (token economy, efficient compression)

These things are "hard" in some sense for LLMs, since they have sliding finite context windows for memories rather than human-like deep recall.

- Boilerplate (imports, class scaffolding, semicolons) wastes tokens
- Implicit state and side effects requires simulating execution
- Distant syntactic dependencies, like matching braces 200 lines apart
- Naming inconsistencies, like `getString`, `get_string`, `GetString`
- Separation of spec from implementation

I had a great working session with Claude on this.  What we devised has echoes of APL and PlankalkÃ¼l.  (PlankalkÃ¼l is a strange duck:  in some way, it's the first programming language, having been devised by Konrad Zuse in the late 1930s for his machine.  It featured a two-dimensional syntax and only had a primitive data type of a single bit.)

Some other desiderata for the language:

1. Homoiconic.  Code is data.
2. Concatenative.  Stack-based computation.  (At this point, it's like a Forth-y Lisp-y APL.)
3. Shape-first types.  Types are tensor shapes + algebraic constraints.  `[3 4] â†’ [4 5] â†’ [3 5]`  is a type signature.
4. Specâ€“implementation unification.  There are not separate test files.  The contracts are the function.  Pre- and post-conditions generate tests automatically.
5. Unicode operators.  Single glyphs describe common operations.  An IDE can convert digraphs to glyphs seamlessly (following Mathematica).  Think of `âˆ˜` compose, `âŠ›` apply, `âŠ—` product, `âŠ•` sum, `â†¦` map, `â¤‡` bind, `âŠ¢` assert/precondition, `âŠ¨` satisfies/postcondition.  Operator overloading is supported for language extension.
6. Explicit effects.  The language is pure by default.  Effects are capabilities passed in for I/O, mutation, randomness handled in types.
7. Structural not textual.  The AST is the primary representation.  Syntax is a serialization format rather than the canonical source of truth.

In working with Nock and writing https://nock.is, I have thought extensively about combinators and their relationship to systems.  Essentially, a set of combinators is the fundamental toolkit for carving reality at the joints.  Thus one of the most important things to get right is the set of primitives.

So, ladies and gentlemen, please meet (for the first time in our material plane), the `goth` language.  `goth` is a homoiconic, statically-typed language targeting the LLVM IR (actually MLIR).

```
# Dot product of two double-precision vectors
â•­â”€ dot : [n]F64 â†’ [n]F64 â†’ F64
â•°â”€ â‚€ âŠ— â‚ Î£
```

Here's a multi-line function with conditions:

```
goth[0]â€º
â•­â”€ normalize : [n]F â†’ [n]F
â”‚  âŠ¢ len â‚€ > 0
â”‚  âŠ¨ abs(norm â‚€ - sqrt(len â‚)) < 0.0001
â•°â”€ let arr â† â‚€ ;
       n â† len arr ;
       Î¼ â† sum arr / n ;
       Ïƒ â† sqrt(sum ((arr â†¦ (Î»â†’ â‚€ - Î¼)) â†¦ (Î»â†’ â‚€ Ã— â‚€)) / n)
   in (arr â†¦ (Î»â†’ â‚€ - Î¼)) â†¦ (Î»â†’ â‚€ / Ïƒ)
goth[1]â€º normalize([1.0, 2.0, 3.0, 4.0, 5.0])
[-1.414213562373095 -0.7071067811865475 0 0.7071067811865475 1.414213562373095]
goth[2]â€º norm(normalize([1.0, 2.0, 3.0, 4.0, 5.0]))
2.2360679774997894
```

Here is a higher-order function:
```
let xs â† [1 2 3] in
  # Map a function over xs
  xs â†¦ (Î»â†’ â‚€ Ã— 2)
```

These are equivalents of each other, defining an outer product:
```
let matrix_op â† Î»â†’ Î»â†’ â‚€ + â‚ Ã— â‚‚ in
#                    â†‘   â†‘   â†‘
#                    inner Î» arg (row element)
#                        outer Î» arg (row)  
#                            matrix_op's own captured... wait

# Clearer:
â•­â”€ outer : A â†’ (B â†’ C)
â•°â”€ Î»â†’ â‚€ + â‚
#     â†‘   â†‘
#     inner arg (B)
#         outer arg (A)
```

## Primitive Operators

| Glyph | Name          | Semantics           |
| ----- | ------------- | ------------------- |
| `â†¦`   | map           | functor lift        |
| `â¤‡`   | bind          | monadic chain       |
| `âˆ˜`   | compose       | f âˆ˜ g = Î»x. f(g(x)) |
| `â–¸`   | filter        | predicate select    |
| `âŠ•`   | sum           | coproduct / concat  |
| `âŠ—`   | product       | tensor / zip        |
| `Î£`   | fold          | reduce with +       |
| `Î `   | fold          | reduce with Ã—       |
| `âŠ¢`   | precondition  | must hold on entry  |
| `âŠ¨`   | postcondition | must hold on exit   |
| `â–¡`   | pure          | no effects          |
| `â—‡`   | effect        | capability required |

What do you get from this set?  Here's what Claude and I like about it:
- APL-like battle-tested array of tooling.
- Highly dense tokens.  Expressing an algorithm with fewer tokens means that the LLM can see more program in context.  Dense symbolic notation reduces what we could call â€œparse loadâ€.
- Unambiguous parsing.  No lookahead is necessary and there are no context-sensitive grammar quirks.
- The spec is the test.  Correct code is generated by algebraically satisfying the postcondition.
- Shape errors will be caught statically:  `[3 4] â†’ [5 5]` is a type error which is instantly legible.
- Compositional reasoning mirrors function pipelines naturally.

Efficient semantic compression takes place along three axes:
1. Elision, what can be inferred.  Implicit is better than explicit when inference can take place trivially.  Effects are pure unless annotated.  Arguments are positionally marked with de Bruijn indices rather than by names.
2. Factoring, shared structure (write-once code).  The AST is a DAG, not a tree.  Common subexpressions get names or indices once.  If multiple references occur we reference rather than redefining.  Macros can quote and unquote.
3. Density, more meaning per glyph.  In fact, we can even have expansions while still preserving semantic atoms.  `âŠ›Î£` is a single unit (a catamorphism)

| Pattern       | Glyph | Expansion      |
| ------------- | ----- | -------------- |
| map-reduce    | `âŠ›Î£`  | `â†¦ f âˆ˜ Î£`      |
| filter-map    | `â–¸â†¦`  | `â–¸ p âˆ˜ â†¦ f`    |
| zip-with      | `âŠ—f`  | `zip âˆ˜ â†¦ f`    |
| scan          | `â€`   | prefix fold    |
| outer product | `âˆ˜.`  | APL-style      |
| broadcast     | `âº`   | shape coercion |

The thing to track isn't tokens per se, but "bits" per semantic unit.  The relevant semantic units are things like function application, binding introduction, type constraint, shape assertion, effect annotation, and so forth.  The sort-of goal, in that it's hyperreal, is to approach the Kolmogorov complexity for algorithms.  Most languages build quicksort in 50â€“100 semantic units, but if we can whittle that down to, say, 20 then we're really cooking with gas.

## Type System

Next, let's look at some data representations.  Tensors are the universal container:
```
[1 2 3]           # vector, shape [3]
[[1 2] [3 4]]     # matrix, shape [2 2]
[2 3 4]âŠ¢0         # zeros, shape [2 3 4]
[2 3 4]âŠ¢Ï         # random, shape [2 3 4], requires â—‡rand
```

Records are positional tuples with optional labels for humans (since we're using Unicode we aren't limited to overloading square brackets):
```
âŸ¨42, "alice", âŠ¤âŸ©           # tuple
âŸ¨age: 42, name: "alice"âŸ©   # labeled (sugar, compiles to positional)
```

```
# Definition
Point â‰¡ âŸ¨x: F64, y: F64âŸ©

# Construction
p â† âŸ¨x: 3.0, y: 4.0âŸ©

# Access (by name for humans, compiles to positional)
p.x        # or pâ‚€
p.y        # or pâ‚

# Update (functional)
pâŠ¢{x: 5.0}   # returns new Point with x changed
```

Variants are sum types:
```
âŸ¨L x | R yâŸ©                # either
âŸ¨None | Some vâŸ©            # option
# Pattern match: âŠ³ L â†’ f | R â†’ g
```

Typeclasses are behavioral interfaces:
```
class Num Ï„ where
  (+) : Ï„ â†’ Ï„ â†’ Ï„
  (Ã—) : Ï„ â†’ Ï„ â†’ Ï„
  zero : Ï„
  one : Ï„

impl Num F64 where
  (+) â† float_add
  (Ã—) â† float_mul
  zero â† 0.0
  one â† 1.0

impl Num [n]F64 where
  (+) â† âŠ•         # elementwise
  (Ã—) â† âŠ—         # elementwise
  zero â† [n]âŠ¢0
  one â† [n]âŠ¢1
```

(Why no objects?  A few reasons:
1. Method dispatch is implicit control flow (which is hard for LLMs to trace)
2. Inheritance creates non-local reasoning
3. Typeclasses are explicit: you see the constraint, you know the interface
4. Records + typeclasses cover the same ground with more clarity
Remember, we're building this for a context window.)

Finally, if the grammar is a value in the language, then we can type macros:
```
Grammar â† âŸ¨
  Expr  â†’ âŸ¨Lam Expr | App Expr Expr | Idx â„• | Lit Val | Op PrimâŸ©
  Val   â†’ âŸ¨Tensor Shape Data | Tuple [Val] | ...âŸ©
  Shape â†’ [â„•]
  Prim  â†’ âŸ¨+ | Ã— | â†¦ | Î£ | ...âŸ©
âŸ©
```

Tensors are homogeneous containers with a shape. The element type is parametric:

```
[n]T        # vector of n elements of type T
[m n]T      # matrix
[a b c]T    # 3-tensor

T âˆˆ { F64, F32, I64, U8, Bool, Char, ... , âŸ¨UserTypeâŸ© }
```

Primitive types include things like these:

| Type   | Repr                         | Notes            |
| ------ | ---------------------------- | ---------------- |
| `Fx`   | IEEE 754 (or something else) | x = 16,32,64,128 |
| `Ix`   | signed int                   | x = 8,16,32,64   |
| `Ux`   | unsigned                     | x = 8,16,32,64   |
| `Bool` | 1 bit                        | packed in arrays |
| `Char` | Unicode scalar               | 32-bit           |
| `Byte` | U8                           | raw bytes        |

Strings are `[n]Char`.  (`Char` is a 32-bit Unicode scalar (UCS-4).)  UTF-8 encoding is an explicit conversion to bytes, not the internal representation:
```
"hello" : [5]Char
"hello"âŠ¢utf8 : [5]Byte   # 5 bytes

"hÃ©llo" : [5]Char        # 5 codepoints
"hÃ©llo"âŠ¢utf8 : [6]Byte   # 6 bytes (Ã© = 2 bytes)

"ğŸ”¥" : [1]Char           # 1 codepoint  
"ğŸ”¥"âŠ¢utf8 : [4]Byte      # 4 bytes
```

Compound element types via tuples:
```
# Array of 3D points
[100]âŸ¨F64, F64, F64âŸ©

# AoS â†” SoA is a view transformation
[100]âŸ¨F64, F64, F64âŸ© âŠ¢soa â‰¡ âŸ¨[100]F64, [100]F64, [100]F64âŸ©
```

Byte arrays for foreign data:
```
[n]Byte              # raw buffer
[n]Byte âŠ¢as [m]F32   # reinterpret (requires n = 4m)
```

Some other bits and pieces:
```
= value equality (vanilla comparison)
â‰¡ structural equality (Î±-equivalent, ignoring sharing)
â‰£ referential equality (same node in the DAG)
```

```
3 + 4 = 7        # âŠ¤ : Bool (runtime comparison)

Point â‰¡ âŸ¨x: F64, y: F64âŸ©    # type alias (compile-time)

let a â† [1 2 3] in
let b â† a in
  a â‰£ b          # âŠ¤ (same reference)
  a = [1 2 3]    # âŠ¤ (same value)
  a â‰£ [1 2 3]    # âŠ¥ (different nodes)
```

Arguments are indicated positionally using De Bruijn indices, `â‚€ â‚ â‚‚ ...`.  Let's look at our first function definition thus far:

```
â•­â”€ normalize : [n]F â†’ [n]F
â”‚  âŠ¨ â€–resultâ€– = 1
â•°â”€ let Î¼ â† Î£ â‚€ / n ;
       Ïƒ â† âˆš(Î£ (â‚€ - Î¼)Â² / n)
   in (â‚€ - Î¼) / Ïƒ
```

De Bruijn indices only work from 0â€“9 so for high-arity functions you switch to explicit naming:
```
Î»âŸ¨a b c d e f g h i j kâŸ©â†’ ...
```

Subscripts nest:
```
Î»â†’ Î»â†’ â‚€ + â‚
#      â†‘   â†‘
#      inner outer
```

`â‚€` is always the innermost binding.

The language is homoiconic, so `âŸ¨âŸ©` quotes while `â€¹â€º` unquotes (splices).

There is a terse form and an explicit form.  This echoes Hoon's tall form and wide form.  These are examples of equivalent pairs:

```
# Terse: filter primes, square, sum
xs â–¸prime â†¦(Â²) Î£

# Explicit: same thing
do xs
  â–¸ Î»â†’ prime? â‚€
  â†¦ Î»â†’ â‚€ Ã— â‚€
  Î£
end

# Hybrid: terse with one explicit lambda
xs â–¸prime â†¦(Î»â†’ â‚€ Ã— â‚€) Î£
```

```
# Nested map-reduce
data â†¦(â†¦f Î£) Î£      # sum of row sums after mapping f

# Clearer with block
do data
  â†¦ do â‚€
       â†¦ f
       Î£
     end
  Î£
end
```

Casting types:

- `âŠ¢as`   compile-time view cast (zero-cost, shape proven)
- `âŠ¢as?`  runtime view cast (returns Option, validates)
- `âŠ¢as!`  runtime view cast (panics on failure)

This function parses a runtime wire:
```
â•­â”€ parse_matrix : [n]Byte â†’ ?âŸ¨[r c]F32, ParseErrorâŸ©
â”‚  â—‡io
â•°â”€ do â‚€
     # Read header (first 8 bytes = two U32 for dimensions)
     âŠ¢slice[0:8] âŠ¢as? âŸ¨U32, U32âŸ© â†’ âŸ¨r, câŸ©
     
     # Validate remaining length
     âŠ¢ n = 8 + 4Ã—rÃ—c
     
     # Cast body
     âŠ¢slice[8:] âŠ¢as? [r c]F32
   end
```

Type-level existentials for unknown shapes:
```
âˆƒn. [n]F32     # "some array of floats, length unknown"
```

Once you validate, you can "open" the existential:
```
â•­â”€ process : (âˆƒn. [n]F32) â†’ F64
â•°â”€ unpack [n]xs â† â‚€ in    # n now in scope as type variable
     xs Î£ / n
```

The unpack binds both the shape witness `n` and the value `xs`.

Foreign buffers with lifetime:
```
[n]ByteâŠ¢foreignâŸ¨'aâŸ©    # borrowed from FFI, lifetime 'a
```

No copies until you explicitly `âŠ¢clone`.  This lifetime management prevents use-after-free.

## Interval Arithmetic and Error Propagation

This is where it gets powerful. An interval type tracks the _range_ of possible values:

```
# Type says: returns float in [0, 1]
â•­â”€ sigmoid : F64 â†’ F64âŠ¢[0..1]
â•°â”€ 1 / (1 + e^(-â‚€))

# Type says: array index is in valid range
â•­â”€ safe_index : [n]Ï„ â†’ U64âŠ¢[0..n) â†’ Ï„
â•°â”€ â‚€[â‚]   # no bounds check needed, proven safe
```

**Interval arithmetic propagates:**

```
x : F64âŠ¢[0..1]
y : F64âŠ¢[0..1]

x + y : F64âŠ¢[0..2]      # addition
x Ã— y : F64âŠ¢[0..1]      # multiplication
x - y : F64âŠ¢[-1..1]     # subtraction
1/x   : F64âŠ¢[1..âˆ]      # requires xâŠ¢(0..1], prevents div-by-zero
```

**Error propagation** (this is the formal methods angle):

```
# Measured value with uncertainty
Î¼ Â± Ïƒ  â‰¡  F64âŠ¢[Î¼-3Ïƒ..Î¼+3Ïƒ]   # 3-sigma interval

# Operations propagate uncertainty
â•­â”€ add_uncertain : (F64 Â± F64) â†’ (F64 Â± F64) â†’ (F64 Â± F64)
â•°â”€ (â‚€.Î¼ + â‚.Î¼) Â± âˆš(â‚€.ÏƒÂ² + â‚.ÏƒÂ²)
```

Or more elegantly, make `Â±` a type constructor:

```
Ï„Â±  â‰¡  âŸ¨Î¼: Ï„, Ïƒ: Ï„âŠ¢[0..âˆ)âŸ©   # value with uncertainty

# Arithmetic on uncertain values is overloaded
(+) : F64Â± â†’ F64Â± â†’ F64Â±
```

**Compile-time bounds checking:**

```
â•­â”€ mat_index : [m n]Ï„ â†’ U64âŠ¢[0..m) â†’ U64âŠ¢[0..n) â†’ Ï„
â•°â”€ â‚€[â‚, â‚‚]

# Usage
A : [100 200]F64
i : U64 â† user_input    # type is U64âŠ¢[0..âˆ)

A[i, 0]                  # TYPE ERROR: i not proven < 100

# Fix: validate
if i < 100 then
  # here i : U64âŠ¢[0..100)
  A[i, 0]                # OK
else
  error "out of bounds"
```

The conditional _refines_ the type in each branch. This is flow-sensitive typing.

### Constraints (Type Classes)

```
where Ï„: Ord          # has ordering
where Ï„: Num          # has +, Ã—, 0, 1
where Ï„: Ring         # Num + negation
where Ï„: Field        # Ring + division
where Ïƒ: Broadcastable Ïƒ'   # shapes can broadcast
where n > 0           # shape constraint
where n = m Ã— k       # shape equality
```

**Example with multiple constraints:**

```
â•­â”€ normalize : [n]Ï„ â†’ [n]Ï„
â”‚  where Ï„: Field
â”‚  where n > 0
â”‚  âŠ¨ â€–resultâ€– = 1
â•°â”€ â‚€ / â€–â‚€â€–
```

### Refinement Types

The `{x : Ï„ | P(x)}` form lets you embed arbitrary predicates:

```
# Sorted array
Sorted [n]Ï„  â‰¡  {xs : [n]Ï„ | âˆ€i j. i < j â†’ xs[i] â‰¤ xs[j]}

# Non-empty
NonEmpty [n]Ï„  â‰¡  {xs : [n]Ï„ | n > 0}

# Probability distribution (sums to 1)
Prob [n]  â‰¡  {xs : [n]F64âŠ¢[0..1] | Î£xs = 1}
```

**Spec integration:**

```
â•­â”€ qsort : [n]Ï„ â†’ Sorted [n]Ï„
â”‚  where Ï„: Ord
â”‚  âŠ¨ permutation â‚€ result
â•°â”€ ...
```

The return type `Sorted [n]Ï„` is a refinement. The postcondition `âŠ¨ permutation` adds another constraint. The compiler/prover must verify both.

## Effects

### Effect Rows

Effects compose:

```
â–¡              pure, no effects
â—‡io            I/O (file, network, console)
â—‡mut           local mutation
â—‡rand          randomness (requires seed/RNG)
â—‡div           possible divergence (non-termination)
â—‡exnâŸ¨EâŸ©        may throw exception of type E
â—‡ffiâŸ¨'aâŸ©       foreign call with lifetime

# Combined
â—‡io âˆª â—‡rand    I/O and randomness
```

**Effect polymorphism:**

```
â•­â”€ map : (Ï„ â†’ ÏƒâŠ¢Îµ) â†’ [n]Ï„ â†’ [n]ÏƒâŠ¢Îµ
â•°â”€ ...

# If the mapped function is pure, map is pure
# If the mapped function has â—‡io, map has â—‡io
```

### Summary: A Typed Expression

```
â•­â”€ train_step : Model â†’ Batch â†’ ModelâŠ¢â—‡rand
â”‚  where Model: Differentiable
â”‚  âŠ¢ batch.size > 0
â”‚  âŠ¨ loss(result, batch) â‰¤ loss(â‚€, batch)   # loss decreases
â•°â”€ let grads â† âˆ‡(loss, â‚€, â‚) ;
       lr â† 0.01 Â± 0.001                      # uncertain learning rate
   in â‚€ - lr Ã— grads
```

This says:

- Takes Model and Batch, returns Model
- Requires randomness effect
- Model must be differentiable (has gradient)
- Batch must be non-empty
- Postcondition: loss improves
- Implementation uses interval arithmetic on learning rate

## Comments

```
# line comment

#[ 
   block comment
   #[ nestable ]#
]#

#- disabled expression -#   # preserves AST node, marked inactive
```

Example:
```
â•­â”€ f : T â†’ T
â•°â”€ impl_v2 â‚€
   #- impl_v1 â‚€ -#    # still in AST, not executed
```

## Compiling (& Compression Redux)

The AST is the source of truth.  Text representations can vary:

```
# Dense (for LLM consumption, remove all whitespace)
â•­â”€f:[n]Tâ†’[n]Tâ•°â”€â‚€â†¦(Î»â†’â‚€Ã—2)

# Pretty-printed (for human reading)
â•­â”€ f : [n]T â†’ [n]T
â•°â”€ â‚€ â†¦ (Î»â†’ â‚€ Ã— 2)

# Verbose (for pedagogy)
â•­â”€ f : [n]T â†’ [n]T
â•°â”€ â‚€ â†¦ (
     Î»â†’ â‚€ Ã— 2
   )
```

There are three equivalent representations in the system:

|Format|Extension|Use case|
|---|---|---|
|Text|`.goth`|Human authoring/reading|
|JSON AST|`.gast`|Tooling, diffs, transforms|
|Binary AST|`.gbin`|LLM I/O, compilation, storage|

These are isomorphic and the compiler will accept any of them.  Instead of emitting text and hoping it parses:
```
"â•­â”€ f : [n]T â†’ [n]T\nâ•°â”€ â‚€ â†¦ (Î»â†’ â‚€ Ã— 2)"
```

the LLM can emit a structured AST directly (here JSON but can also be binary):
```
{
  "fn": {
    "sig": {"args": [{"tensor": {"shape": ["n"], "elem": "T"}}], 
            "ret": {"tensor": {"shape": ["n"], "elem": "T"}}},
    "body": {"app": [{"op": "map"}, 
                     {"idx": 0}, 
                     {"lam": {"app": [{"op": "mul"}, {"idx": 0}, {"lit": 2}]}}]}
  }
}
```

The parser is really a textâ†’AST deserializer.  The prettyprinter goes from ASTâ†’text.  Both are trivial once you have the AST spec.

Here's what the compilation pipeline looks like, in that case:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             GOTH TOOLCHAIN              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      
â”‚  .goth   â”‚  â”‚ .gast   â”‚  â”‚ .gbin   â”‚    â† Source formats  
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      
     â”‚             â”‚             â”‚                           
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚    GOTH AST   â”‚  â† Canonical in-memory repr    
           â”‚     (DAG)     â”‚                                
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”                   
           â”‚   Typecheck   â”‚â†â”€â”€â†’â”‚  Z3   â”‚  â† SMT for        
           â”‚   + Constrain â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜    intervals,     
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 shapes,        
                   â†“                         refinements    
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚   Typed AST   â”‚  â† All types resolved          
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚   Monomorph   â”‚  â† Specialize generics         
           â”‚   + Closure   â”‚    Convert closures            
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚    GOTH MIR   â”‚  â† Low-level, explicit         
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚      MLIR     â”‚  â† tensor, affine, scf         
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚    LLVM IR    â”‚                                
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                
                   â†“                                        
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
           â”‚    Machine    â”‚                                
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Finally, we're going to integrate SMT Z3 into the compiler pipeline so that we have proofs over the AST statements at compile time.

The ultimate goal is for `goth` to not only be bootstrapping but generates its own macros and DSLs seamlessly.

